{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_ver1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepthi-skive/gpt-2/blob/master/GPT2_ver1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Wfx_44enhm8",
        "outputId": "32278e4c-ca3b-40cb-ee9d-6704330bb7d4"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEQ55aHvno2C",
        "outputId": "a6625e28-bab8-4e8a-8328-a14047196d73"
      },
      "source": [
        "# Create gpt-2 folder \r\n",
        "#Run only once\r\n",
        "%cd /content/drive/My\\ Drive/\r\n",
        "!mkdir gpt-2\r\n",
        "%cd gpt-2/\r\n",
        "# Clone GPT2 repo from OPenAI github\r\n",
        "!git clone https://github.com/openai/gpt-2.git \r\n",
        "# Change directory to gpt-2\r\n",
        "%cd cd gpt-2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n",
            "/content/drive/My Drive/gpt-2\n",
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 233 (delta 0), reused 1 (delta 0), pack-reused 230\u001b[K\n",
            "Receiving objects: 100% (233/233), 4.38 MiB | 11.86 MiB/s, done.\n",
            "Resolving deltas: 100% (121/121), done.\n",
            "[Errno 2] No such file or directory: 'cd gpt-2'\n",
            "/content/drive/My Drive/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVWfzu2SoPqb",
        "outputId": "c8f03350-269a-4b55-a36f-9db51cdb3fd7"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/gpt-2/gpt-2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/gpt-2/gpt-2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4v5tRnZoW3f",
        "outputId": "32a595d8-055c-4afd-9cb7-5e682701d02a"
      },
      "source": [
        "# Change tensorflow version to 1. \r\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yawEXJ2_odWg",
        "outputId": "eafd648e-4300-4b50-ebba-410751edfbf3"
      },
      "source": [
        "# Install requirement.txt\r\n",
        "!pip3 install -r requirements.txt\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fire>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.3.1)\n",
            "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2017.4.5)\n",
            "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
            "Requirement already satisfied: tqdm==4.31.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.31.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2020.12.5)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTnjwu8yoirO",
        "outputId": "8e352872-c58d-4c1a-ec9e-0487771e7d86"
      },
      "source": [
        "import time\r\n",
        "t0= time.clock()\r\n",
        "print(\"Importing Models\")\r\n",
        "\r\n",
        "#Importing all 4 versions of GPT2 model\r\n",
        "!python3 download_model.py 124M\r\n",
        "!python3 download_model.py 355M\r\n",
        "!python3 download_model.py 774M\r\n",
        "!python3 download_model.py 1558M\r\n",
        "\r\n",
        "t1 = time.clock() - t0\r\n",
        "print(\"Time elapsed: \", t1) # CPU seconds elapsed to download models"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importing Models\n",
            "Fetching checkpoint: 1.00kit [00:00, 859kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 2.86Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 855kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:15, 33.1Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 4.61Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 1.74Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 1.76Mit/s]                                                       \n",
            "Fetching checkpoint: 1.00kit [00:00, 855kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 3.03Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 854kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [01:04, 21.9Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 7.07Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 3.05Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 1.92Mit/s]                                                       \n",
            "Fetching checkpoint: 1.00kit [00:00, 862kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 2.90Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 826kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 3.10Git [01:34, 32.9Mit/s]                                 \n",
            "Fetching model.ckpt.index: 16.0kit [00:00, 11.4Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 1.38Mit [00:00, 2.94Mit/s]                                                \n",
            "Fetching vocab.bpe: 457kit [00:00, 2.29Mit/s]                                                       \n",
            "Fetching checkpoint: 1.00kit [00:00, 841kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 2.85Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 788kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 6.23Git [05:40, 18.3Mit/s]                                 \n",
            "Fetching model.ckpt.index: 21.0kit [00:00, 14.8Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 1.84Mit [00:00, 3.85Mit/s]                                                \n",
            "Fetching vocab.bpe: 457kit [00:00, 1.87Mit/s]                                                       \n",
            "Time elapsed:  5.591453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFj6h1kQpThd",
        "outputId": "9bc94e87-010e-4791-8918-f910fceb0f51"
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py -- --help"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mNAME\u001b[0m\n",
            "    interactive_conditional_samples.py - Interactively run the model :model_name=124M : String, which model to use :seed=None : Integer seed for random number generators, fix seed to reproduce results :nsamples=1 : Number of samples to return total :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples. :length=None : Number of tokens in generated text, if None (default), is determined by model hyperparameters :temperature=1 : Float value controlling randomness in boltzmann distribution. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions. :top_k=0 : Integer value controlling diversity. 1 means only 1 word is considered for each step (token), resulting in deterministic completions, while 40 means 40 words are considered at each step. 0 (default) is a special setting meaning no restrictions. 40 generally is a good value. :models_dir : path to parent folder containing model subfolders (i.e. contains the <model_name> folder)\n",
            "\n",
            "\u001b[1mSYNOPSIS\u001b[0m\n",
            "    interactive_conditional_samples.py <flags>\n",
            "\n",
            "\u001b[1mDESCRIPTION\u001b[0m\n",
            "    Interactively run the model :model_name=124M : String, which model to use :seed=None : Integer seed for random number generators, fix seed to reproduce results :nsamples=1 : Number of samples to return total :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples. :length=None : Number of tokens in generated text, if None (default), is determined by model hyperparameters :temperature=1 : Float value controlling randomness in boltzmann distribution. Lower temperature results in less random completions. As the temperature approaches zero, the model will become deterministic and repetitive. Higher temperature results in more random completions. :top_k=0 : Integer value controlling diversity. 1 means only 1 word is considered for each step (token), resulting in deterministic completions, while 40 means 40 words are considered at each step. 0 (default) is a special setting meaning no restrictions. 40 generally is a good value. :models_dir : path to parent folder containing model subfolders (i.e. contains the <model_name> folder)\n",
            "\n",
            "\u001b[1mFLAGS\u001b[0m\n",
            "    --model_name=\u001b[4mMODEL_NAME\u001b[0m\n",
            "    --seed=\u001b[4mSEED\u001b[0m\n",
            "    --nsamples=\u001b[4mNSAMPLES\u001b[0m\n",
            "    --batch_size=\u001b[4mBATCH_SIZE\u001b[0m\n",
            "    --length=\u001b[4mLENGTH\u001b[0m\n",
            "    --temperature=\u001b[4mTEMPERATURE\u001b[0m\n",
            "    --top_k=\u001b[4mTOP_K\u001b[0m\n",
            "    --top_p=\u001b[4mTOP_P\u001b[0m\n",
            "    --models_dir=\u001b[4mMODELS_DIR\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfoPzWvVo3Rj",
        "outputId": "ad612366-11ca-4b33-d140-3c24f8f90add"
      },
      "source": [
        "!python3 src/interactive_conditional_samples.py --top_k 40"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:57: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2020-12-11 20:30:41.447843: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-12-11 20:30:41.491566: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2020-12-11 20:30:41.491624: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (2e86652b0bfb): /proc/driver/nvidia/version does not exist\n",
            "2020-12-11 20:30:41.506406: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2020-12-11 20:30:41.507033: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14d92c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-12-11 20:30:41.507076: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:58: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:60: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/gpt-2/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/gpt-2/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/gpt-2/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/gpt-2/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/gpt-2/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/gpt-2/gpt-2/src/sample.py:16: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/drive/My Drive/gpt-2/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From src/interactive_conditional_samples.py:68: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "2020-12-11 20:30:46.328687: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 154389504 exceeds 10% of system memory.\n",
            "Model prompt >>> Today in popular news,\n",
            "2020-12-11 20:31:51.083996: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 22560768 exceeds 10% of system memory.\n",
            "2020-12-11 20:31:51.162855: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 22634496 exceeds 10% of system memory.\n",
            "2020-12-11 20:31:51.242076: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 22708224 exceeds 10% of system memory.\n",
            "2020-12-11 20:31:51.325683: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 22781952 exceeds 10% of system memory.\n",
            "======================================== SAMPLE 1 ========================================\n",
            " President Obama is making a big push to crack down on illicit drug sales. There's another big national issue to keep an eye on.<|endoftext|>In this week's installment of our series, we're going to look at the first five games of 2017.\n",
            "\n",
            "If you're into the big game, I can say that you're probably on your own. With that in mind we'll get into a couple of interesting matchups and look at how those matchup shapes up. But first we're going to dive into the very top-tier teams who are playing the biggest games of the year.\n",
            "\n",
            "And when I say it in general terms, this article is intended to focus on the top-tier teams. It's not meant to be a list of top teams that they're going to crush, like Team Liquid or Fnatic. This list is for everyone who's playing well enough to make it and those who aren't so far behind.\n",
            "\n",
            "Team Liquid\n",
            "\n",
            "We'll do all sorts of research for you, so if you're a good player and have done some pretty good work on your team you may well find this article an excellent read. If your game is one that deserves attention – you should check into the top teams, because that's the way the world works. If you're one who's playing in a competitive league such as the LCS, we know some top-level players who'll play well but if that's not the case then we're pretty sure that what you're reading here will be of little value for you.\n",
            "\n",
            "In any case you might be surprised to learn that as of just two weeks ago, Natus Vincere had a record-breaking 3-3 record in the LCK with five losses. This was achieved with seven losses to eight. As a result, it seemed a big upset for North America and European teams if they failed to score a single point in the LCK, so if you're looking forward to seeing how you perform at this tournament you've gotta check out our other articles. For now the only thing that really matters to us is who's who. You're welcome, it's all good.\n",
            "\n",
            "Natus Vincere\n",
            "\n",
            "The team that played Natus Vincere very well in 2016 hasn't really been playing well in 2017. It can't be said that they have made big changes in 2017, with the return of Cloud9 (which they won at the World Championship) and the return of the mid laner Kreygasm. But what makes Natus Vincere different\n",
            "================================================================================\n",
            "Model prompt >>> In all fairness,\n",
            "======================================== SAMPLE 1 ========================================\n",
            " he was a good player in both leagues: he never broke through for a top prospect, despite being able to win MVP votes. I didn't see much of him that season when I watched him on a few fantasy TV shows -- it wasn't one of those games where you want teams to win and you're playing that with the ball rolling in your hands. He was a great player with his own style of play -- though there is no question that he had a good year, in particular. If that is where the Cowboys would lean on him, he would be worth a lot of more.\n",
            "\n",
            "4. Derek Carr - QB, Los Angeles Raiders\n",
            "\n",
            "As mentioned, there were two of the Raiders' first three drafts for Carr to get that draft pick. His first was a \"pick 2\" and was in place until the end of the season. The second one was \"pick 6.\" That draft selection came in an interesting draft for the Raiders: the Raiders had no picks on the first four or so teams. So, with the Raiders taking a number on Carr, he had the chance to make a run in the top 15 for a few teams. In the end, that took a bit more than the rest. He played great in the NFL, but was also never really a top-25 quarterback in any of his career.\n",
            "\n",
            "That decision came in the offseason. With one year remaining in his rookie contract and his one year of being suspended for the first half of the season, it was decided to start off the season with a four-year, $40.12 million contract. A lot of that was about not rushing for more than 30 yards more than he had in his first two seasons, which is just one part of what made the Raiders' offseason so much special. A lot of that was on the defensive side of the ball, which allowed Carr to play one of the best defensive ends in the NFL, Reggie Bush, during the offseason, and in the run game. They were also able to draft him on the day he missed six of his first seven starts and he made that decision without much thought for the rest of the year -- the Raiders made it a whole lot easier to draft him. Still not too impressed by why they didn't do it, and it certainly shouldn't be.\n",
            "\n",
            "The reason why they didn't did that is that, in spite of how great Derek Carr played his first two seasons, he never really found his niche. In the end, his performance was mostly down to poor decision\n",
            "================================================================================\n",
            "Model prompt >>> "
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}